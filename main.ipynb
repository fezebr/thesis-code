{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7fa4_kTprKa",
        "outputId": "82cf8926-49fc-4e91-c5c2-d15fd00a180e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "standard = StandardScaler()\n",
        "minmax = MinMaxScaler()\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "dataset = \"./train_FD001.txt\"\n",
        "hierarchicalPath= \"./hierarchical_100_unit.pkl\"\n",
        "output_dir= './results/v-20'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ltODKyg8p295"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_full = pd.read_csv( dataset, sep=\" \", header=None, skipinitialspace=True).dropna(axis=1)\n",
        "df_full = df_full.rename(columns={0: 'unit', 1: 'cycle', 2: 'W1', 3: 'W2', 4: 'W3'})\n",
        "# Filter df_full for engine units 1 to 5\n",
        "# df_full = df_full[df_full['unit'].isin(range(1,26))].reset_index(drop=True)\n",
        "df_full = df_full.reset_index(drop=True)\n",
        "\n",
        "df = df_full\n",
        "df_full.columns = df_full.columns.astype(str)\n",
        "\n",
        "df_A = df[df.columns[[0, 1]]] # unit and cycle\n",
        "df_W = df[df.columns[[2, 3, 4]]] # W1 w2 w3 (opration conditions)\n",
        "df_S = df[df.columns[list(range(5, 26))]] #sensors\n",
        "df_X = pd.concat([df_W, df_S], axis=1)\n",
        "\n",
        "df[['W1', 'W2', 'W3', *map(str, range(5, 26))]] = minmax.fit_transform(pd.concat([df_W, df_S], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RRTSY52np8oy"
      },
      "outputs": [],
      "source": [
        "\n",
        "cols_to_drop = df.nunique()[df.nunique() == 1].index\n",
        "df = df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "cols_to_drop = df.nunique()[df.nunique() == 2].index\n",
        "df = df.drop(cols_to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eNdpJlDLl1F",
        "outputId": "853b9f75-0710-4564-9e20-ee7ce41233db"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(hierarchicalPath, 'rb') as f:\n",
        "    hierarchical = pickle.load(f)\n",
        "\n",
        "print(hierarchical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert strings to integers if needed\n",
        "hierarchical = [[int(x) for x in arr] for arr in hierarchical]\n",
        "\n",
        "# Now compute the differences\n",
        "differences = [arr[-1] - arr[0] for arr in hierarchical]\n",
        "print(differences)\n",
        "\n",
        "# Plotting\n",
        "sorted_differences = sorted(differences)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(sorted_differences)), sorted_differences, color='skyblue')\n",
        "plt.title('Bar Chart of Sorted Index Differences (Last - First)')\n",
        "plt.xlabel('Sorted Array Index')\n",
        "plt.ylabel('Index Difference')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- ŸÜŸÖŸàÿØÿßÿ± ŸÅÿ±ÿßŸàÿßŸÜ€å (Ÿá€åÿ≥ÿ™Ÿà⁄Øÿ±ÿßŸÖ) ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(differences, bins=20, color='orange', edgecolor='black')\n",
        "plt.title('Histogram of Index Differences')\n",
        "plt.xlabel('Index Difference (Last - First)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8saW9hH9vh-b"
      },
      "source": [
        "<hr/>\n",
        "RL\n",
        "<hr/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "lRuk4JAivquD",
        "outputId": "727b44a4-1337-4887-adb2-e7c0a1635247"
      },
      "outputs": [],
      "source": [
        "df_X[['W1', 'W2', 'W3', *map(str, range(5, 26))]] = minmax.fit_transform(pd.concat([df_X], axis=1))\n",
        "cols_to_drop = df_X.nunique()[df_X.nunique() == 1].index\n",
        "df_X = df_X.drop(cols_to_drop, axis=1)\n",
        "df_X\n",
        "# df_hmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "0cT5guvevtWC"
      },
      "outputs": [],
      "source": [
        "c_f = -30\n",
        "c_r = -10\n",
        "do_nothing = 0\n",
        "policy = {}\n",
        "policy_test = {}\n",
        "engine_unit = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "3n65fNiEvwmc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class CustomEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, is_training=True, verbose=False, nn='dnn', input_data='raw', srla=True):\n",
        "        self.train = is_training\n",
        "        self.verbose = verbose\n",
        "        self.nn = nn\n",
        "        self.input_data = input_data\n",
        "        self.srla = srla\n",
        "        \n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(df_X.shape[1],))\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.reward = 0\n",
        "        self.done = False\n",
        "        self.engine_unit = engine_unit\n",
        "        self.cycle = int(hierarchical[self.engine_unit - 1][0]) - 1\n",
        "        self.engine_df_A = df_A[df_A['unit'] == self.engine_unit]\n",
        "         \n",
        "        self.X = df_X[self.engine_df_A.index[0]:self.engine_df_A.index[-1] + 1]\n",
        " \n",
        "       \n",
        "        self.state =  self.X.iloc[self.cycle]\n",
        "        self.failure_state = self.engine_df_A['cycle'].max() - 1\n",
        "\n",
        "    def get_next_engine_data(self ,x_engine_unit):\n",
        "        self.engine_unit = x_engine_unit\n",
        "\n",
        "\n",
        "        self.engine_df_A = df_A[df_A['unit'] == self.engine_unit]\n",
        "        self.X = df_X[self.engine_df_A.index[0]:self.engine_df_A.index[-1] + 1]\n",
        "\n",
        "        self.failure_state = self.engine_df_A['cycle'].max() - 1\n",
        "        return self.X\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            if self.verbose:\n",
        "                print(\"|hold|:\", self.cycle)\n",
        "            if self.cycle == self.failure_state:\n",
        "                fraction = 1 / (int(hierarchical[self.engine_unit - 1][-1]) - int(hierarchical[self.engine_unit - 1][0]))\n",
        "                self.reward = (c_r + c_f) /  math.exp(fraction)\n",
        "               \n",
        "                self.state =  self.X.iloc[self.cycle]\n",
        "                self.done = True\n",
        "\n",
        "                if self.train:\n",
        "                    policy[self.engine_unit] = {'unit': self.engine_unit,\n",
        "                                                'failure_state': self.failure_state,\n",
        "                                                'replace_state': self.cycle}\n",
        "                else:\n",
        "                    policy_test[self.engine_unit] = {'unit': self.engine_unit,\n",
        "                                                     'failure_state': self.failure_state,\n",
        "                                                     'replace_state': self.cycle,\n",
        "                                                     'reward': self.reward}\n",
        "                if self.verbose:\n",
        "                    print(\"|cycle reached failure state|:\", self.cycle, \"reward:\", self.reward, '\\n')\n",
        "            else:\n",
        "                self.reward = do_nothing\n",
        "                self.cycle += 1\n",
        "                if self.srla:\n",
        "\n",
        "                    if self.cycle in hierarchical[self.engine_unit - 1]:\n",
        "                  \n",
        "                        self.state =  self.X.iloc[self.cycle]\n",
        "                        self.done = False\n",
        "                        if self.verbose:\n",
        "                            print(\"Danger zone!\",'\\n')\n",
        "                            print(\"|system running|\", \"reward:\", self.reward, '\\n')\n",
        "                    else:\n",
        "                        # print('*****pass srla*****')\n",
        "                        pass\n",
        "                else:\n",
        "                   \n",
        "                    self.state =  self.X.iloc[self.cycle]\n",
        "                    self.done = False\n",
        "                    if self.verbose:\n",
        "                        print(\"|system running|\", \"reward:\", self.reward, '\\n')\n",
        "        elif action == 1:\n",
        "            if self.verbose:\n",
        "                print(\"|replace|:\", self.cycle)\n",
        "            if self.cycle == self.failure_state:\n",
        "                fraction = 1 / (int(hierarchical[self.engine_unit - 1][-1]) - int(hierarchical[self.engine_unit - 1][0]))\n",
        "                self.reward = (c_r + c_f) /  math.exp(fraction)\n",
        "                # self.reward = (c_r + c_f) / (int(hierarchical[self.engine_unit - 1][-1]) - int(hierarchical[self.engine_unit - 1][0]))\n",
        "            else:\n",
        "\n",
        "                self.reward = c_r * (int(hierarchical[self.engine_unit - 1][-1]) - self.cycle) /( int(hierarchical[self.engine_unit - 1][-1]) - int(hierarchical[self.engine_unit - 1][0]))\n",
        " \n",
        "            self.state =  self.X.iloc[self.cycle]\n",
        "            if self.train:\n",
        "                policy[self.engine_unit] = {'unit': self.engine_unit,\n",
        "                                            'failure_state': self.failure_state,\n",
        "                                            'replace_state': self.cycle}\n",
        "            else:\n",
        "                policy_test[self.engine_unit] = {'unit': self.engine_unit,\n",
        "                                                 'failure_state': self.failure_state,\n",
        "                                                 'replace_state': self.cycle,\n",
        "                                                 'reward': self.reward}\n",
        "                print(policy_test[self.engine_unit])\n",
        "            self.done = True\n",
        "        if self.verbose:\n",
        "            print(\"reward:\", self.reward, '\\n')\n",
        "            print(\"******************************************************************\")\n",
        "        info = {}\n",
        "        return self.state, self.reward, self.done, info\n",
        "\n",
        "    def reset(self , engine_unit):\n",
        "        self.X = self.get_next_engine_data(engine_unit)\n",
        "        # self.cycle = 166\n",
        "        self.cycle = int(hierarchical[self.engine_unit - 1][0]) - 1\n",
        "  \n",
        "        self.state =  self.X.iloc[self.cycle]\n",
        "        self.done = False\n",
        "        return self.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "2kM3DIkXvzUh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import gym\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-4,\n",
        "                 batch_size=256, buffer_size=5000, epsilon_start=1.0,\n",
        "                 epsilon_end=0, epsilon_decay=0.995, target_update=240):\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.target_update = target_update\n",
        "        self.step_count = 0\n",
        "        self.losses = []\n",
        "\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.policy_net = QNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = QNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions)\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
        "            target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "        self.losses.append(loss.item())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        return loss.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct5gV2WhTiiM",
        "outputId": "8883a1f4-5d15-457c-a82b-3cc4ca4b6e46"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(666)  # For reproducibility\n",
        "\n",
        "all_indices = list(range(1, 101))\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "# Choose first N for training\n",
        "train_indices = all_indices[:80]\n",
        "\n",
        "# Remaining go to testing\n",
        "test_indices = all_indices[80:100]  # or use random.sample(all_indices[80:], 20) if you want random 20 from rest\n",
        "\n",
        "print('len train_indices', len(train_indices))\n",
        "print('train_indices', train_indices)\n",
        "print('--------------------------------')\n",
        "print('len test_indices', len(test_indices))\n",
        "print('test_indices', test_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igu47Rc-QEIZ",
        "outputId": "3c05ee4f-b2ec-45c7-9e26-dfafdc99352b"
      },
      "outputs": [],
      "source": [
        "# Repeated engine_unit list\n",
        "repeated_list = train_indices * 3700  # Make 105,000-length list\n",
        "chunk_size = len(train_indices)\n",
        "print(chunk_size)\n",
        "for i in range(0, len(repeated_list), chunk_size):\n",
        "    chunk = repeated_list[i:i + chunk_size]\n",
        "    random.shuffle(chunk)\n",
        "    repeated_list[i:i + chunk_size] = chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(repeated_list[0:80])\n",
        "print(repeated_list[80:160])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "episodLength = len(repeated_list)\n",
        "episodLength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "_ygmv87y6SPC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "episode_rewards = []\n",
        "failure_states = []\n",
        "episode_losses = []\n",
        "episode_policy = []\n",
        "epsilon_min_reached_at = None\n",
        "\n",
        "\n",
        "\n",
        "def train_dqn(env, agent, episodes=500):\n",
        "    global epsilon_min_reached_at\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        engine_unit = repeated_list[ep]\n",
        "        state = env.reset(engine_unit)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        losses = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # Example: force action=0 if not in scheduled cycle\n",
        "            if env.cycle not in hierarchical[env.engine_unit - 1]:\n",
        "                action = 0\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            loss = agent.train_step()\n",
        "\n",
        "            if loss is not None:\n",
        "                losses.append(loss)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        failure_states.append(env.cycle)\n",
        "        avg_loss = np.mean(losses) if losses else 0.0\n",
        "        episode_losses.append(avg_loss)\n",
        "\n",
        "        if (ep + 1) % 200 == 0:\n",
        "            agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
        "\n",
        "        if epsilon_min_reached_at is None and agent.epsilon < 0.01:\n",
        "            epsilon_min_reached_at = ep\n",
        "            agent.epsilon = 0\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Episode {ep+1}, Total Reward: {total_reward:.4f}, \"\n",
        "              f\"Epsilon: {agent.epsilon:.6f}, Avg Loss: {avg_loss:.6f} \")\n",
        "        print({\n",
        "            'unit': env.engine_unit,\n",
        "            'failure_state': env.failure_state,\n",
        "            'replace_state': env.cycle\n",
        "        })\n",
        "        episode_policy.append({\n",
        "            'unit': env.engine_unit,\n",
        "            'failure_state': env.failure_state,\n",
        "            'replace_state': env.cycle,\n",
        "            'Epsilon': agent.epsilon,\n",
        "            'avg_loss': int(avg_loss),\n",
        "        })\n",
        "        print('---------------------------------------------------------')\n",
        "\n",
        "    \n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "AqGlQpQfM-Mn"
      },
      "outputs": [],
      "source": [
        "def plot_failure_states(failure_states, engine_unit, epsilons, save_path=None):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    episodes = list(range(1, len(failure_states) + 1))\n",
        "    min_threshold = int(hierarchical[engine_unit - 1][0])\n",
        "    max_threshold = int(hierarchical[engine_unit - 1][-1])\n",
        "\n",
        "    colors = []\n",
        "    for f in failure_states:\n",
        "        if f == max_threshold:\n",
        "            colors.append('red')\n",
        "        elif f == max_threshold - 1 or f == max_threshold - 2:\n",
        "            colors.append('green')\n",
        "        else:\n",
        "            colors.append('blue')\n",
        "\n",
        "    epsilon_min_reached_at = None\n",
        "    for i, e in enumerate(epsilons):\n",
        "        if abs(e) < 1e-8:\n",
        "            epsilon_min_reached_at = i + 1\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.scatter(episodes, failure_states, color=colors, s=10, label=f'Failure States (Unit {engine_unit})')\n",
        "\n",
        "    if epsilon_min_reached_at is not None:\n",
        "        plt.axvline(x=epsilon_min_reached_at, color='orange', linestyle='--',\n",
        "                    label=f'œµ = 0 at Episode {epsilon_min_reached_at}')\n",
        "\n",
        "    plt.axhline(min_threshold, color='red', linestyle='--', label=f'Min Threshold ({min_threshold})')\n",
        "    plt.axhline(max_threshold, color='green', linestyle='--', label=f'Max Threshold ({max_threshold})')\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Failure State (Cycles)')\n",
        "    plt.title(f'Failure State per Episode - Unit {engine_unit}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQwutGl4nRkB",
        "outputId": "ebdd9276-b7a6-4791-8eed-b47bd62f870a"
      },
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "failure_states = []\n",
        "epsilon_min_reached_at = None\n",
        "\n",
        "env = CustomEnv(is_training=True, verbose=False ,  srla=True)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "train_dqn(env, agent, episodes=episodLength)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mean_rewards():\n",
        "    rewards_per_30 = [np.mean(episode_rewards[i:i+80]) for i in range(0, len(episode_rewards), 80)]\n",
        "    x = range(1, len(rewards_per_30) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))  # ÿπÿ±€åÿ∂‚Äåÿ™ÿ± ÿ®ÿ±ÿß€å ÿØ€åÿØ ÿ®Ÿáÿ™ÿ±\n",
        "    plt.plot(x, rewards_per_30, color='royalblue', linewidth=2, label='Mean Reward')\n",
        "    # plt.scatter(x, rewards_per_30, color='orange', s=25, alpha=0.7)  # s=ÿßŸÜÿØÿßÿ≤Ÿá‚Äå€å ŸÜŸÇÿ∑Ÿá‚ÄåŸáÿß\n",
        "\n",
        "    plt.xlabel('Epoch (every 30 episodes)', fontsize=12)\n",
        "    plt.ylabel('Mean Reward', fontsize=12)\n",
        "    plt.title('Mean Reward Over Time (every 80 episodes)', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_mean_rewards()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mean_rewards():\n",
        "    rewards_per_30 = [np.mean(episode_rewards[i:i+80]) for i in range(0, len(episode_rewards), 80)]\n",
        "    x = range(1, len(rewards_per_30) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))  # ÿπÿ±€åÿ∂‚Äåÿ™ÿ± ÿ®ÿ±ÿß€å ÿØ€åÿØ ÿ®Ÿáÿ™ÿ±\n",
        "    plt.plot(x, rewards_per_30, color='royalblue', linewidth=2, label='Mean Reward')\n",
        "    # plt.scatter(x, rewards_per_30, color='orange', s=25, alpha=0.7)  # s=ÿßŸÜÿØÿßÿ≤Ÿá‚Äå€å ŸÜŸÇÿ∑Ÿá‚ÄåŸáÿß\n",
        "\n",
        "    plt.xlabel('Epoch (every 30 episodes)', fontsize=12)\n",
        "    plt.ylabel('Mean Reward', fontsize=12)\n",
        "    plt.title('Mean Reward Over Time (every 80 episodes)', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_mean_rewards()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mean_rewards():\n",
        "    rewards_per_30 = [np.mean(episode_rewards[i:i+80]) for i in range(0, len(episode_rewards), 80)]\n",
        "    x = range(1, len(rewards_per_30) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))  # ÿπÿ±€åÿ∂‚Äåÿ™ÿ± ÿ®ÿ±ÿß€å ÿØ€åÿØ ÿ®Ÿáÿ™ÿ±\n",
        "    plt.plot(x, rewards_per_30, color='royalblue', linewidth=2, label='Mean Reward')\n",
        "    # plt.scatter(x, rewards_per_30, color='orange', s=25, alpha=0.7)  # s=ÿßŸÜÿØÿßÿ≤Ÿá‚Äå€å ŸÜŸÇÿ∑Ÿá‚ÄåŸáÿß\n",
        "\n",
        "    plt.xlabel('Epoch (every 30 episodes)', fontsize=12)\n",
        "    plt.ylabel('Mean Reward', fontsize=12)\n",
        "    plt.title('Mean Reward Over Time (every 80 episodes)', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_mean_rewards()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4P0H7ERNCeS"
      },
      "source": [
        "save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mean_rewards():\n",
        "    rewards_per_30 = [np.mean(episode_rewards[i:i+80]) for i in range(0, len(episode_rewards), 80)]\n",
        "    x = range(1, len(rewards_per_30) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(x, rewards_per_30, color='royalblue', linewidth=2, label='Mean Reward')\n",
        "    \n",
        "    plt.xlabel('Epoch (every 30 episodes)', fontsize=12)\n",
        "    plt.ylabel('Mean Reward', fontsize=12)\n",
        "    plt.title('Mean Reward Over Time (every 80 episodes)', fontsize=14)\n",
        "    plt.ylim(-4, -1)  # üëà This line zooms the y-axis\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_mean_rewards()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "7FDJN7T1NDAx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Group failure_states and epsilons by unit\n",
        "unit_data = defaultdict(lambda: {'failures': [], 'epsilons': []})\n",
        "\n",
        "for record in episode_policy:\n",
        "    unit = record['unit']\n",
        "    unit_data[unit]['failures'].append(record['replace_state'])\n",
        "    unit_data[unit]['epsilons'].append(record['Epsilon'])\n",
        "\n",
        "# Create folder if not exists\n",
        "# output_dir = './results/v-12'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Plot per unit and save\n",
        "for unit, data in unit_data.items():\n",
        "    save_path = os.path.join(output_dir, f'unit_{unit}.png')\n",
        "    plot_failure_states(\n",
        "        failure_states=data['failures'],\n",
        "        engine_unit=unit,\n",
        "        epsilons=data['epsilons'],\n",
        "        save_path=save_path\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F4Nfl2P6WJa",
        "outputId": "f00b460b-2b8c-42b3-c763-511f7641e5b9"
      },
      "outputs": [],
      "source": [
        "policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEEe4Xmh8oaB",
        "outputId": "412c7259-88b9-4490-c5b2-0c7aa8030a62"
      },
      "outputs": [],
      "source": [
        "for k, v in policy.items():\n",
        "    v['RUL'] = v['failure_state'] - v['replace_state']\n",
        "\n",
        "# Optional: pretty print\n",
        "from pprint import pprint\n",
        "pprint(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_zero_rul = sum(1 for v in policy.values() if v['RUL'] == 0)\n",
        "print(\"Number of rows with RUL == 0:\", count_zero_rul)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT-1AZOuDt61",
        "outputId": "a89a88a2-b964-4b13-a668-0201d9b113a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "mean_rul = np.mean([v['RUL'] for v in policy.values()])\n",
        "print(\"Mean RUL:\", mean_rul)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyC5GnEVDzgn",
        "outputId": "ef0728d9-992f-4c6d-c9b2-a4a7fe7ba69c"
      },
      "outputs": [],
      "source": [
        "test_rewards = []\n",
        "test_policy = []\n",
        "\n",
        "# Set env to evaluation mode\n",
        "test_env = CustomEnv(is_training=False, verbose=False, srla=True)\n",
        "\n",
        "for unit in test_indices:\n",
        "    state = test_env.reset(unit)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)  # Use greedy action (no epsilon)\n",
        "\n",
        "        # Optional: Force hold if outside danger zone\n",
        "        if test_env.cycle not in hierarchical[test_env.engine_unit - 1]:\n",
        "            action = 0\n",
        "\n",
        "        next_state, reward, done, _ = test_env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    test_rewards.append(total_reward)\n",
        "    test_policy.append(policy_test[test_env.engine_unit])  # saved inside env.step()\n",
        "\n",
        "    print(f\"Test Unit {unit}, Total Reward: {total_reward:.4f}\")\n",
        "    print(policy_test[test_env.engine_unit])\n",
        "    print('---------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x-TUbY1OGty",
        "outputId": "dcaddf07-d263-43b1-bf42-5d928c571b16"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(test_policy)\n",
        "\n",
        "# Add RUL column\n",
        "df['RUL'] = df['failure_state'] - df['replace_state']\n",
        "\n",
        "# Display the table\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glp65joVL3Q7",
        "outputId": "bf95677f-91e2-4a2f-fac6-7f5e58dd20a3"
      },
      "outputs": [],
      "source": [
        "rul_results = [{'unit': d['unit'], 'RUL': d['failure_state'] - d['replace_state']} for d in test_policy]\n",
        "\n",
        "rul_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSNzXH6FMBCV",
        "outputId": "d7bba728-c7bc-4009-80ca-899115ce8bee"
      },
      "outputs": [],
      "source": [
        "mean_rul = sum(r['RUL'] for r in rul_results) / len(rul_results)\n",
        "\n",
        "print(f\"Mean RUL: {mean_rul}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_zero_rul = sum(1 for r in rul_results if r['RUL'] == 0)\n",
        "print(\"Number of entries with RUL == 0:\", count_zero_rul)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
